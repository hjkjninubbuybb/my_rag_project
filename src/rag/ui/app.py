"""
Gradio UI â€” å…¨æ–° All-in-One ç°ä»£åŒ–ä¸­æ§å° (100% å®Œæ•´ç‰ˆ)ã€‚

é›†æˆä¸‰å¤§æ ¸å¿ƒå·¥ä½œåŒºï¼š
1. ğŸ—„ï¸ çŸ¥è¯†åº“ç®¡çº¿ (åŸºç¡€èµ„äº§ç®¡ç†: å¢åˆ æ”¹æŸ¥)
2. ğŸ”¬ æ¶ˆèå®éªŒå¤§ç›˜ (ç­–ç•¥ç”Ÿæˆ -> æ‰¹é‡å…¥åº“ -> æ‰¹é‡è¯„æµ‹ -> å¯è§†åŒ–)
3. ğŸ¤– Agent è°ƒè¯•èˆ± (åŠ¨æ€å¯¹è¯ä¸å†…éƒ¨çŠ¶æ€ç™½ç›’é€è§†)
"""

import asyncio
import json
import shutil
import traceback
from pathlib import Path
from typing import List

import gradio as gr
import pandas as pd
from langchain_core.messages import HumanMessage, BaseMessage

from rag.config.experiment import ExperimentConfig, ExperimentGrid
from rag.config.settings import settings
from rag.pipeline.ingestion import IngestionService
from rag.storage.vectordb import VectorStoreManager
from rag.storage.metadata import DatabaseManager
from rag.agent.workflow import create_graph
from rag.experiment.runner import BatchExperimentRunner
from rag.experiment.results import ResultsCollector

# æš‚å­˜åŒºè·¯å¾„
UPLOAD_DIR = Path("data/uploads/temp_batch")

def _get_default_config() -> ExperimentConfig:
    return settings.to_experiment_config()

def _get_kb_config(collection_name: str) -> ExperimentConfig:
    """æ ¹æ® collection åç§°æ„å»ºä¸€ä¸ªåŠ¨æ€çš„ ExperimentConfigã€‚"""
    cfg = _get_default_config()
    # ç»•è¿‡ dataclass çš„ frozen é™åˆ¶åˆ›å»ºæ–°å®ä¾‹
    return ExperimentConfig(
        **{**cfg.to_full_dict(), "collection_name_override": collection_name}
    )

def _get_managers(collection_name: str):
    """è·å–æŒ‡å®š Collection çš„å‘é‡åº“å’Œå…³ç³»å‹æ•°æ®åº“ç®¡ç†å™¨ã€‚"""
    cfg = _get_kb_config(collection_name)
    store = VectorStoreManager(cfg)
    # å¤„ç† SQLite è·¯å¾„
    db_path_str = cfg.qdrant_path.replace("vectordb", "metadata.db").replace("data/data/", "data/")
    db = DatabaseManager(db_path=db_path_str, collection_name=collection_name)
    return cfg, store, db


def _sanitize_state_for_json(state: dict) -> dict:
    """å°† LangGraph State è½¬æ¢ä¸º JSON å¯åºåˆ—åŒ–çš„å­—å…¸ã€‚

    messages ä¸­çš„ BaseMessage å¯¹è±¡æ— æ³•ç›´æ¥åºåˆ—åŒ–ï¼Œéœ€è¦æå– role + contentã€‚
    """
    safe = {}
    for key, value in state.items():
        if key == "messages":
            safe[key] = [
                {
                    "role": type(m).__name__,
                    "content": str(m.content)[:500] if hasattr(m, "content") else str(m)[:500],
                }
                for m in value
            ]
        else:
            try:
                json.dumps(value)
                safe[key] = value
            except (TypeError, ValueError):
                safe[key] = str(value)
    return safe

def _extract_content(chunk) -> str:
    """ä» AIMessageChunk æå–æ–‡æœ¬ contentï¼ˆå…¼å®¹ str å’Œ list ä¸¤ç§æ ¼å¼ï¼‰ã€‚"""
    if chunk is None:
        return ""
    content = getattr(chunk, "content", "")
    if isinstance(content, str):
        return content
    if isinstance(content, list):
        return "".join(c.get("text", "") for c in content if isinstance(c, dict))
    return ""


def _build_chunks_df(chunks_data: list) -> pd.DataFrame:
    """å°† debug_retrieved_chunks è½¬ä¸º DataFrameï¼ŒæŒ‰ score é™åºæ’åˆ—ã€‚"""
    if not chunks_data:
        return pd.DataFrame(columns=["score", "source_file", "text"])
    df = pd.DataFrame(chunks_data)
    if {"score", "source_file", "text"}.issubset(df.columns):
        return df.sort_values("score", ascending=False)[["score", "source_file", "text"]]
    return pd.DataFrame(columns=["score", "source_file", "text"])


def format_citations(chunks: list) -> str:
    """å°†æ£€ç´¢å—åˆ—è¡¨æ ¼å¼åŒ–ä¸º Markdown å¼•ç”¨æ¥æºæ®µè½ã€‚åŒä¸€æ–‡ä»¶ä¿ç•™æœ€é«˜åˆ†çš„å—ã€‚"""
    if not chunks:
        return ""
    seen: dict = {}
    for chunk in chunks:
        fname = chunk.get("source_file", "æœªçŸ¥æ–‡ä»¶")
        score = float(chunk.get("score", 0.0))
        if fname not in seen or score > seen[fname]["score"]:
            seen[fname] = {"score": score, "text": chunk.get("text", "")}

    lines = ["\n\n---\n**å‚è€ƒæ¥æº**\n"]
    for i, (fname, info) in enumerate(seen.items(), 1):
        excerpt = info["text"][:150].replace("\n", " ").strip()
        lines.append(f"{i}. ğŸ“„ **{fname}** (ç›¸å…³åº¦: {info['score']:.3f})")
        lines.append(f"   > {excerpt}â€¦")
    return "\n".join(lines)


def create_ui():
    # è¿è¡Œæ—¶å…¨å±€çŠ¶æ€
    current_config: List[ExperimentConfig] = []
    results_collector = ResultsCollector()

    # é¢„åŠ è½½ configs/ ç›®å½•ä¸‹çš„å‘½åé…ç½®æ–‡ä»¶ (best_retrieval.yaml ç­‰)
    _preset_dir = Path("configs")
    _preset_files = ["best_retrieval.yaml"]
    for _fname in _preset_files:
        _fpath = _preset_dir / _fname
        if _fpath.exists():
            try:
                _cfg = ExperimentConfig.from_yaml(str(_fpath), api_key=settings.dashscope_api_key or "")
                current_config.append(_cfg)
                print(f"âœ… [UI] é¢„è®¾é…ç½®å·²åŠ è½½: {_cfg.experiment_id}")
            except Exception as _e:
                print(f"âš ï¸ [UI] é¢„è®¾é…ç½®åŠ è½½å¤±è´¥ ({_fname}): {_e}")

    _initial_dropdown_choices = ["default"] + [c.experiment_id for c in current_config]

    with gr.Blocks(title="Agentic RAG ä¸­æ§å°", theme=gr.themes.Soft(), fill_width=True) as demo:

        with gr.Row():
            # ==========================================
            # â¬…ï¸ å·¦ä¾§ï¼šå…¨å±€ä¾§è¾¹æ  (Global Sidebar)
            # ==========================================
            with gr.Column(scale=1, min_width=280):
                gr.Markdown("### âš™ï¸ å…¨å±€æ§åˆ¶é¢æ¿")

                global_config_selector = gr.Dropdown(
                    choices=_initial_dropdown_choices,
                    value="default",
                    label="ğŸŸ¢ å½“å‰æ¿€æ´»çš„å®éªŒé…ç½® (Agent ä½¿ç”¨)",
                    interactive=True
                )

                gr.Markdown("---")
                gr.Markdown("#### ğŸ—ƒï¸ çŸ¥è¯†åº“å…¨å±€çŠ¶æ€")
                kb_collection_name = gr.Textbox(
                    value=settings.collection_name,
                    label="ä¸»æ§ Collection åç§°",
                    interactive=False
                )
                kb_status_md = gr.Markdown("**æš‚å­˜åŒºæ–‡ä»¶:** 0 ä¸ª\n**éœ€å‰å¾€å³ä¾§ [çŸ¥è¯†åº“ç®¡çº¿] åˆ·æ–°è¯¦ç»†æ•°æ®**")

                gr.Markdown("---")
                global_log = gr.Textbox(label="ç³»ç»Ÿå®æ—¶æ—¥å¿—", lines=15, interactive=False, text_align="left")

            # ==========================================
            # â¡ï¸ å³ä¾§ï¼šæ ¸å¿ƒå·¥ä½œåŒº (Main Workspaces)
            # ==========================================
            with gr.Column(scale=4):
                with gr.Tabs():

                    # ---------------- å·¥ä½œåŒº 1ï¼šçŸ¥è¯†åº“ç®¡çº¿ (æ—¥å¸¸èµ„äº§ç®¡ç†) ----------------
                    with gr.Tab("ğŸ—„ï¸ çŸ¥è¯†åº“ç®¡çº¿", id="tab_kb"):
                        gr.Markdown("### åŸºç¡€çŸ¥è¯†åº“ç®¡ç† (å¢åˆ æ”¹æŸ¥)")
                        gr.Markdown("æ­¤å¤„ç”¨äºç®¡ç†ä¸»é›†åˆï¼ˆDefault Collectionï¼‰çš„æ–‡æ¡£èµ„äº§ã€‚å®éªŒé›†åˆçš„æ•°æ®åœ¨[æ¶ˆèå¤§ç›˜]ä¸­è‡ªåŠ¨ç”Ÿæˆã€‚")

                        with gr.Row():
                            with gr.Column(scale=1):
                                gr.Markdown("#### 1. å¾…å¤„ç†æš‚å­˜åŒº")
                                staging_files = gr.File(label="æ‹–æ‹½ PDF/MD åˆ°æ­¤å¤„", file_count="multiple")
                                with gr.Row():
                                    upload_btn = gr.Button("ä¸Šä¼ è‡³æš‚å­˜åŒº", variant="secondary")
                                    clear_staging_btn = gr.Button("æ¸…ç©ºæš‚å­˜åŒº", variant="stop")
                                run_single_ingest_btn = gr.Button("ğŸš€ å¤„ç†æš‚å­˜åŒº (å•åº“å…¥åº“)", variant="primary")

                            with gr.Column(scale=1):
                                gr.Markdown("#### 2. å·²å…¥åº“èµ„äº§ (Qdrant & SQLite)")
                                db_file_list = gr.CheckboxGroup(label="å·²ç´¢å¼•æ–‡æ¡£åˆ—è¡¨", choices=[], interactive=True)
                                with gr.Row():
                                    refresh_db_btn = gr.Button("ğŸ”„ åˆ·æ–°åˆ—è¡¨")
                                    delete_db_btn = gr.Button("ğŸ—‘ï¸ åˆ é™¤é€‰ä¸­", variant="stop")

                        kb_action_log = gr.Textbox(label="èµ„äº§æ“ä½œæ—¥å¿—", lines=3, interactive=False)

                    # ---------------- å·¥ä½œåŒº 2ï¼šæ¶ˆèå®éªŒå¤§ç›˜ (æ ¸å¿ƒè¯„æµ‹) ----------------
                    with gr.Tab("ğŸ”¬ æ¶ˆèå®éªŒå¤§ç›˜", id="tab_eval"):
                        gr.Markdown("### ç­–ç•¥çŸ©é˜µç”Ÿæˆä¸æ‰¹é‡åŸºå‡†æµ‹è¯•")

                        with gr.Row():
                            with gr.Column(scale=1):
                                gr.Markdown("#### 1. ç”Ÿæˆå®éªŒçŸ©é˜µ")
                                chunk_strategies = gr.CheckboxGroup(choices=["fixed", "recursive", "sentence"], value=["fixed", "recursive"], label="åˆ‡ç‰‡ç­–ç•¥")
                                chunk_sizes = gr.CheckboxGroup(choices=["128", "256", "512"], value=["256"], label="Chunk Size")
                                chunk_overlaps = gr.CheckboxGroup(choices=["25", "50"], value=["50"], label="Overlap")
                                hybrid_opts = gr.CheckboxGroup(choices=["True", "False"], value=["True", "False"], label="Hybrid Search")
                                merge_opts = gr.CheckboxGroup(choices=["True", "False"], value=["True", "False"], label="Auto Merge")
                                rerank_opts = gr.CheckboxGroup(choices=["True", "False"], value=["True", "False"], label="Rerank")
                                generate_grid_btn = gr.Button("ç”Ÿæˆå®éªŒçŸ©é˜µ â¬‡ï¸")
                                config_preview = gr.Dataframe(label="æ‰§è¡Œé˜Ÿåˆ—é¢„è§ˆ", interactive=False, max_height=200)

                            with gr.Column(scale=2):
                                gr.Markdown("#### 2. æ‰§è¡Œæ‰¹é‡ç®¡çº¿")
                                with gr.Row():
                                    run_batch_ingest_btn = gr.Button("ğŸ“¦ 1. æ™ºèƒ½å…¥åº“ (æŒ‰çŸ©é˜µæ„å»ºå¤šåº“)", variant="secondary")
                                    run_batch_eval_btn = gr.Button("â–¶ï¸ 2. è¿è¡Œè¯„æµ‹", variant="primary")

                                with gr.Row():
                                    dataset_path = gr.Textbox(value="tests/data/test_dataset.csv", label="è¯„æµ‹é›†è·¯å¾„", scale=2)
                                    dataset_limit = gr.Slider(minimum=0, maximum=100, step=1, value=10, label="æœ€å¤§æµ‹è¯•æ¡æ•°(0=å…¨é‡)", scale=1)

                                gr.Markdown("#### 3. å®éªŒæ´å¯Ÿ")
                                refresh_plot_btn = gr.Button("ğŸ”„ åˆ·æ–°å¯è§†åŒ–å›¾è¡¨")
                                eval_plot = gr.ScatterPlot(
                                    x="avg_latency_ms", y="hit_rate", color="experiment_id",
                                    title="æ€§èƒ½ vs è´¨é‡æ•£ç‚¹å›¾ (å¸•ç´¯æ‰˜åˆ†å¸ƒ)",
                                    tooltip=["experiment_id", "hit_rate", "ndcg", "avg_latency_ms"],
                                    width=600, height=300
                                )
                                eval_table = gr.Dataframe(label="æ ¸å¿ƒæŒ‡æ ‡å¯¹æ¯”æ¦œå•", interactive=False)

                    # ---------------- å·¥ä½œåŒº 3ï¼šAgent è°ƒè¯•èˆ± (ç™½ç›’æµ‹è¯•) ----------------
                    with gr.Tab("ğŸ¤– Agent è°ƒè¯•èˆ±", id="tab_agent"):
                        gr.Markdown("### æ·±åº¦ç™½ç›’é€è§†ä¸äº¤äº’è¯Šæ–­")

                        with gr.Row():
                            with gr.Column(scale=5):
                                chatbot = gr.Chatbot(label="Agent ä¼šè¯çª—å£", height=500, avatar_images=(None, "ğŸ¤–"))
                                with gr.Row():
                                    msg_input = gr.Textbox(label="è¾“å…¥é—®é¢˜", placeholder="è¯¢é—®å…³äºçŸ¥è¯†åº“çš„å†…å®¹ï¼ŒæŒ‰ Enter å‘é€...", scale=4)
                                    send_btn = gr.Button("å‘é€", variant="primary", scale=1)
                                clear_chat_btn = gr.Button("æ¸…ç©ºä¸Šä¸‹æ–‡", size="sm")

                            with gr.Column(scale=4):
                                gr.Markdown("#### ğŸ” å†…éƒ¨é“¾è·¯è¿½è¸ª (Tracing)")

                                with gr.Accordion("1. æ„å›¾æ”¹å†™ (Query Rewrite)", open=True):
                                    debug_rewrites = gr.JSON(label="è§£æå‡ºçš„æ£€ç´¢è¯ (rewrittenQuestions)")

                                with gr.Accordion("2. ç‰©ç†å¬å›æ¢é’ˆ (Retrieved Chunks)", open=True):
                                    debug_chunks = gr.Dataframe(
                                        label="åº•å±‚å¼•æ“å®é™…å¬å›åˆ†å— (æŒ‰ Score é™åº)",
                                        headers=["score", "source_file", "text"],
                                        wrap=True
                                    )

                                with gr.Accordion("3. LangGraph å®Œæ•´çŠ¶æ€", open=False):
                                    debug_full_state = gr.JSON(label="State å­—å…¸å¿«ç…§")

        # ==========================================
        # é€»è¾‘ç»‘å®š (Event Handlers)
        # ==========================================

        # --- é€šç”¨åŠŸèƒ½ ---
        def _update_staging_files():
            if not UPLOAD_DIR.exists(): return None
            files = [str(f) for f in UPLOAD_DIR.iterdir() if f.is_file()]
            return files if files else None

        # --- å·¥ä½œåŒº 1ï¼šçŸ¥è¯†åº“ç®¡çº¿ ---
        def list_db_files(collection_name):
            try:
                _, _, db = _get_managers(collection_name)
                files = db.get_all_files()
                return gr.update(choices=files, label=f"å·²ç´¢å¼•æ–‡æ¡£åˆ—è¡¨ ({len(files)})")
            except Exception:
                return gr.update(choices=[], label="æ— æ³•åŠ è½½åˆ—è¡¨")

        def handle_upload(files):
            if not files: return None, "è¯·å…ˆé€‰æ‹©æ–‡ä»¶"
            UPLOAD_DIR.mkdir(parents=True, exist_ok=True)
            saved = []
            for file in files:
                target = UPLOAD_DIR / Path(file.name).name
                shutil.copy(file.name, target)
                saved.append(str(target))
            return saved, f"æˆåŠŸä¸Šä¼  {len(saved)} ä¸ªæ–‡ä»¶åˆ°æš‚å­˜åŒºã€‚"

        def clear_staging():
            if UPLOAD_DIR.exists():
                shutil.rmtree(UPLOAD_DIR)
                UPLOAD_DIR.mkdir(parents=True, exist_ok=True)
            return None, "æš‚å­˜åŒºå·²æ¸…ç©ºã€‚"

        async def start_single_ingestion(collection_name):
            if not UPLOAD_DIR.exists() or not any(UPLOAD_DIR.iterdir()):
                return "æš‚å­˜åŒºä¸ºç©ºï¼Œæ— éœ€å…¥åº“ã€‚", list_db_files(collection_name)

            cfg, store, db = _get_managers(collection_name)
            files_to_process = [f.name for f in UPLOAD_DIR.iterdir() if f.is_file()]

            try:
                ingestion = IngestionService(cfg)
                await ingestion.process_directory(str(UPLOAD_DIR))

                # æ›´æ–°å…ƒæ•°æ® SQLite
                for filename in files_to_process:
                    db.add_file(filename)

                # æ¸…ç†æš‚å­˜åŒº
                shutil.rmtree(UPLOAD_DIR)
                UPLOAD_DIR.mkdir(parents=True, exist_ok=True)

                return f"âœ… æˆåŠŸç´¢å¼• {len(files_to_process)} ä¸ªæ–‡ä»¶åˆ° {collection_name}ã€‚", list_db_files(collection_name)
            except Exception as e:
                traceback.print_exc()
                return f"âŒ å¤„ç†å¤±è´¥: {e}", list_db_files(collection_name)

        def delete_from_db(selected_files, collection_name):
            if not selected_files: return "è¯·å…ˆå‹¾é€‰è¦åˆ é™¤çš„æ–‡ä»¶", list_db_files(collection_name)
            cfg, store, db = _get_managers(collection_name)
            for fname in selected_files:
                store.delete_file(fname)
                db.remove_file(fname)
            return f"ğŸ—‘ï¸ å·²æˆåŠŸåˆ é™¤ {len(selected_files)} ä¸ªæ–‡ä»¶ã€‚", list_db_files(collection_name)

        # ç»‘å®š Tab 1 äº‹ä»¶
        demo.load(fn=_update_staging_files, outputs=staging_files)
        demo.load(fn=list_db_files, inputs=[kb_collection_name], outputs=[db_file_list])

        upload_btn.click(fn=handle_upload, inputs=staging_files, outputs=[staging_files, kb_action_log])
        clear_staging_btn.click(fn=clear_staging, outputs=[staging_files, kb_action_log])
        refresh_db_btn.click(fn=list_db_files, inputs=[kb_collection_name], outputs=[db_file_list])
        delete_db_btn.click(fn=delete_from_db, inputs=[db_file_list, kb_collection_name], outputs=[kb_action_log, db_file_list])
        run_single_ingest_btn.click(fn=start_single_ingestion, inputs=[kb_collection_name], outputs=[kb_action_log, db_file_list]).then(fn=_update_staging_files, outputs=staging_files)


        # --- å·¥ä½œåŒº 2ï¼šæ¶ˆèå®éªŒå¤§ç›˜ ---
        def generate_matrix(strats, sizes, overlaps, hybrid, merge, rerank):
            if not all([strats, sizes, overlaps, hybrid, merge, rerank]):
                return pd.DataFrame(), gr.update(), "è¯·ç¡®ä¿æ¯ä¸ªç»´åº¦è‡³å°‘é€‰æ‹©ä¸€ä¸ªé€‰é¡¹ã€‚"

            grid = ExperimentGrid(
                chunking_strategies=strats,
                chunk_sizes_child=[int(s) for s in sizes],
                chunk_overlaps=[int(o) for o in overlaps],
                enable_hybrid=[v == "True" for v in hybrid],
                enable_auto_merge=[v == "True" for v in merge],
                enable_rerank=[v == "True" for v in rerank],
            )
            configs = grid.generate_configs(api_key=settings.dashscope_api_key or "")
            # ä¿ç•™ presetsï¼Œä»…æ›¿æ¢æ¶ˆèå®éªŒé…ç½® (ID ä»¥ "ablation_" å¼€å¤´)
            current_config[:] = [c for c in current_config if not c.experiment_id.startswith("ablation_")]
            current_config.extend(configs)

            dropdown_choices = ["default"] + [c.experiment_id for c in current_config]
            rows = [{
                "ID": c.experiment_id,
                "ç­–ç•¥": c.chunking_strategy,
                "Size": c.chunk_size_child,
                "Hybrid": c.enable_hybrid,
                "Merge": c.enable_auto_merge,
                "Rerank": c.enable_rerank,
            } for c in configs]

            return (
                pd.DataFrame(rows),
                gr.update(choices=dropdown_choices, value=dropdown_choices[1] if len(dropdown_choices) > 1 else "default"),
                f"ç”Ÿæˆäº† {len(configs)} ç»„å®éªŒé…ç½®ã€‚",
            )

        generate_grid_btn.click(fn=generate_matrix, inputs=[chunk_strategies, chunk_sizes, chunk_overlaps, hybrid_opts, merge_opts, rerank_opts], outputs=[config_preview, global_config_selector, global_log])

        async def run_batch_ingestion():
            if not current_config:
                return "è¯·å…ˆåœ¨å·¦ä¾§ç”Ÿæˆé…ç½®çŸ©é˜µã€‚"
            if not UPLOAD_DIR.exists() or not any(UPLOAD_DIR.iterdir()):
                return "æš‚å­˜åŒºä¸ºç©ºï¼Œè¯·å…ˆåœ¨ [çŸ¥è¯†åº“ç®¡çº¿] ä¸Šä¼ æ–‡æ¡£ã€‚"
            logs = ["å¼€å§‹æ‰§è¡Œæ‰¹é‡å®éªŒå…¥åº“..."]
            runner = BatchExperimentRunner(
                configs=current_config,
                dataset_path="",
                input_dir=str(UPLOAD_DIR),
                progress_callback=lambda m: logs.append(m),
            )
            try:
                await runner.run_ingestion()
                logs.append("æ‰¹é‡å…¥åº“å®Œæ¯•ã€‚")
            except Exception as e:
                traceback.print_exc()
                logs.append(f"å…¥åº“å‡ºé”™: {e}")
            return "\n".join(logs)

        run_batch_ingest_btn.click(fn=run_batch_ingestion, outputs=[global_log])

        def run_batch_eval(ds_path, limit):
            if not current_config:
                return "é…ç½®é˜Ÿåˆ—ä¸ºç©ºï¼Œè¯·å…ˆç”ŸæˆçŸ©é˜µã€‚", pd.DataFrame(), pd.DataFrame()
            logs = ["å¼€å§‹æ‰¹é‡è¯„æµ‹..."]
            runner = BatchExperimentRunner(
                configs=current_config,
                dataset_path=ds_path,
                input_dir="",
                progress_callback=lambda m: logs.append(m),
            )
            try:
                summaries, details = runner.run_evaluation(limit=int(limit) if limit > 0 else None)
                if summaries:
                    results_collector.save_batch(summaries, details, tag="ablation")
            except Exception as e:
                traceback.print_exc()
                logs.append(f"è¯„æµ‹å‡ºé”™: {e}")

            df = results_collector.get_comparison_dataframe()
            if df.empty:
                return "\n".join(logs) + "\nè¯„æµ‹å®Œæˆï¼Œä½†æœªäº§ç”Ÿæœ‰æ•ˆæ•°æ®ã€‚", pd.DataFrame(), pd.DataFrame()
            logs.append("è¯„æµ‹æŠ¥å‘Šç”Ÿæˆå®Œæ¯•ã€‚")
            return "\n".join(logs), df, df

        run_batch_eval_btn.click(fn=run_batch_eval, inputs=[dataset_path, dataset_limit], outputs=[global_log, eval_plot, eval_table])
        refresh_plot_btn.click(fn=lambda: (results_collector.get_comparison_dataframe(), results_collector.get_comparison_dataframe()), outputs=[eval_plot, eval_table])


        # --- å·¥ä½œåŒº 3ï¼šAgent è°ƒè¯•èˆ± ---
        chat_state = {"graph": None, "config_id": None}

        _empty_chunks_df = pd.DataFrame(columns=["score", "source_file", "text"])

        async def process_chat(user_msg, chat_history, config_id):
            """æµå¼èŠå¤©å¤„ç†å™¨ (async generator)ã€‚æ¯æ¬¡ yield é©±åŠ¨ Gradio å®æ—¶æ›´æ–°ã€‚"""
            if not user_msg:
                yield chat_history, "", [], _empty_chunks_df, {}
                return

            # 1. æŒ‚è½½ LangGraphï¼ˆç¼“å­˜å¤ç”¨ï¼Œé¿å…é‡å¤åˆå§‹åŒ–ï¼‰
            if chat_state["config_id"] != config_id or chat_state["graph"] is None:
                cfg = next((c for c in current_config if c.experiment_id == config_id), _get_default_config())
                chat_state["graph"] = create_graph(cfg)
                chat_state["config_id"] = config_id

            graph = chat_state["graph"]
            inputs = {"messages": [HumanMessage(content=user_msg)]}
            lc_config = {"configurable": {"thread_id": "debugger_1"}, "recursion_limit": 25}

            current_text = ""
            agent_buffer = ""
            aggregate_started = False

            # 2. å³æ—¶åé¦ˆï¼šæ¸…ç©ºè¾“å…¥æ¡†ï¼Œæ˜¾ç¤ºå…‰æ ‡æŒ‡ç¤ºç¬¦
            yield chat_history + [[user_msg, "â–Œ"]], "", [], _empty_chunks_df, {}

            try:
                # 3. æµå¼æ‰§è¡Œ LangGraph
                async for event in graph.astream_events(inputs, config=lc_config, version="v2"):
                    kind = event.get("event", "")
                    node = event.get("metadata", {}).get("langgraph_node", "")
                    data = event.get("data", {})

                    if kind == "on_chat_model_stream":
                        content = _extract_content(data.get("chunk"))

                        if node == "aggregate" and content:
                            # å¤šé—®é¢˜è·¯å¾„ï¼šè¿½åŠ èšåˆ LLM çš„æ¯ä¸ª token
                            current_text += content
                            yield chat_history + [[user_msg, current_text + "â–Œ"]], "", [], _empty_chunks_df, {}

                        elif node == "agent" and content and not aggregate_started:
                            # å•é—®é¢˜è·¯å¾„ï¼šAgent æœ€ç»ˆæ–‡æœ¬å›å¤ï¼ˆå·¥å…·è°ƒç”¨è½®æ¬¡ content ä¸ºç©ºï¼Œè‡ªç„¶è¿‡æ»¤ï¼‰
                            agent_buffer += content
                            current_text = agent_buffer
                            yield chat_history + [[user_msg, current_text + "â–Œ"]], "", [], _empty_chunks_df, {}

                    elif kind == "on_chat_model_start":
                        if node == "agent":
                            # ReAct æ–°è½®æ¬¡å¼€å§‹ï¼Œé‡ç½® agent å€™é€‰ç¼“å†²
                            agent_buffer = ""
                        elif node == "aggregate":
                            # ç¡®è®¤ä¸ºå¤šé—®é¢˜è·¯å¾„ï¼šåœ¨èšåˆ LLM å¯åŠ¨æ—¶ç«‹å³æ¸…ç©ºè¢«å„å­ Agent æ±¡æŸ“çš„ä¸­é—´è¾“å‡º
                            aggregate_started = True
                            current_text = ""

                # 4. æµç»“æŸï¼šä» MemorySaver checkpointer æ‹‰å–å®Œæ•´æœ€ç»ˆçŠ¶æ€
                final_snapshot = await graph.aget_state(lc_config)
                state_values = final_snapshot.values

                debug_chunks = state_values.get("debug_retrieved_chunks", [])
                rewrites = state_values.get("rewrittenQuestions", ["(æœªè§¦å‘æ„å›¾æ”¹å†™)"])

                # 5. æ ¼å¼åŒ–å¼•ç”¨æ¥æºå¹¶è¿½åŠ åˆ°ç­”æ¡ˆæœ«å°¾
                # å®‰å…¨å…œåº•ï¼šè‹¥æµå¼æœªæ•è·åˆ°ä»»ä½• tokenï¼Œä» State æ¶ˆæ¯åˆ—è¡¨ä¸­æå–æœ€ç»ˆç­”æ¡ˆ
                if not current_text:
                    messages = state_values.get("messages", [])
                    if messages:
                        current_text = getattr(messages[-1], "content", "") or ""

                citations = format_citations(debug_chunks)
                final_text = current_text + citations if citations else current_text

                new_history = chat_history + [[user_msg, final_text]]
                yield new_history, "", rewrites, _build_chunks_df(debug_chunks), _sanitize_state_for_json(state_values)

            except Exception as e:
                traceback.print_exc()
                yield chat_history + [[user_msg, f"å¤„ç†å‡ºé”™: {e}"]], "", [], _empty_chunks_df, {"error": str(e)}

        # ç»‘å®šå¯¹è¯äº‹ä»¶
        for trigger in [msg_input.submit, send_btn.click]:
            trigger(
                fn=process_chat,
                inputs=[msg_input, chatbot, global_config_selector],
                outputs=[chatbot, msg_input, debug_rewrites, debug_chunks, debug_full_state]
            )

        def _clear_chat():
            chat_state["graph"] = None
            chat_state["config_id"] = None
            return [], "", [], pd.DataFrame(columns=["score", "source_file", "text"]), {}

        clear_chat_btn.click(fn=_clear_chat, outputs=[chatbot, msg_input, debug_rewrites, debug_chunks, debug_full_state])

    return demo

if __name__ == "__main__":
    app = create_ui()
    app.launch(server_name="127.0.0.1", server_port=7860)