"""
[Architecture Role: Model Factory (æ¨¡å‹å·¥å‚)]
æ­¤æ¨¡å—è´Ÿè´£ç”Ÿäº§ç³»ç»Ÿæ‰€éœ€çš„æ‰€æœ‰æ¨¡å‹å®ä¾‹ï¼Œå®ç°äº† "å·¥å‚æ¨¡å¼" (Factory Pattern)ã€‚

æ ¸å¿ƒèŒè´£:
1. [LLM & Embedding] ç»Ÿä¸€ç®¡ç†å¤§æ¨¡å‹å’Œå‘é‡æ¨¡å‹çš„åˆå§‹åŒ–å‚æ•°ã€‚
2. [Strategy Dispatch] æ ¹æ®é…ç½®æ–‡ä»¶ (YAML) ä¸­çš„ç­–ç•¥å­—æ®µï¼ŒåŠ¨æ€ç»„è£…ç®—æ³•ç»„ä»¶ã€‚
3. [Singleton Cache] å¯¹é‡å‹æ¨¡å‹ (å¦‚ BGE-M3) è¿›è¡Œå•ä¾‹ç¼“å­˜ï¼Œé¿å…é‡å¤åŠ è½½æ¶ˆè€—æ˜¾å­˜ã€‚

æ ¸å¿ƒå‡çº§ (Phase 4):
- æ–°å¢ `get_text_splitter()` æ–¹æ³•ï¼Œå®ç°äº†æ–‡æœ¬åˆ‡åˆ†ç®—æ³•çš„ç­–ç•¥è·¯ç”± (Routing)ã€‚
- æ”¯æŒ "Fixed Token" (åŸºå‡†) ä¸ "Recursive Character" (è¯­ä¹‰å¢å¼º) ä¸¤ç§ç­–ç•¥çš„æ— ç¼åˆ‡æ¢ã€‚
"""

from typing import Tuple, Callable, List, Any
import traceback

# ğŸ‘‡ LlamaIndex æ ¸å¿ƒç»„ä»¶
# NodeParser æ˜¯æ‰€æœ‰åˆ‡åˆ†å™¨çš„åŸºç±» (Base Class)ï¼Œç”¨äºç±»å‹æç¤º
from llama_index.core.node_parser import NodeParser, TokenTextSplitter, SentenceSplitter
from llama_index.llms.dashscope import DashScope
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.postprocessor.dashscope_rerank import DashScopeRerank
from FlagEmbedding import BGEM3FlagModel

from app.settings import settings

# ğŸ‘‡ å°è¯•å¯¼å…¥ ModelScope (ç”¨äºå›½å†…æé€Ÿä¸‹è½½)
try:
    from modelscope import snapshot_download
    HAS_MODELSCOPE = True
except ImportError:
    HAS_MODELSCOPE = False


class ModelFactory:
    # BGE-M3 æ¨¡å‹ç¼“å­˜ (å•ä¾‹æ¨¡å¼ï¼Œé˜²æ­¢å¤šæ¬¡åŠ è½½çˆ†å†…å­˜)
    _bgem3_cache = None

    @staticmethod
    def get_llm():
        """
        è·å– LLM (å¤§è¯­è¨€æ¨¡å‹) å®ä¾‹
        å½“å‰é…ç½®: é˜¿é‡Œäº‘ Qwen-Plus
        """
        return DashScope(
            model_name=settings.llm_model,
            api_key=settings.dashscope_api_key,
            temperature=0.1  # ä¿æŒä½æ¸©åº¦ä»¥è·å¾—ç¨³å®šçš„äº‹å®æ€§å›ç­”
        )

    @staticmethod
    def get_embedding():
        """
        è·å– Dense Embedding (ç¨ å¯†å‘é‡) æ¨¡å‹
        å½“å‰é…ç½®: é˜¿é‡Œäº‘ text-embedding-v4 (1536ç»´)
        å®ç°: ä½¿ç”¨ OpenAI å…¼å®¹æ¥å£è°ƒç”¨ DashScope
        """
        return OpenAIEmbedding(
            model_name=settings.embedding_model,
            api_key=settings.dashscope_api_key,
            api_base="https://dashscope.aliyuncs.com/compatible-mode/v1",
            embed_batch_size=10,
        )

    @staticmethod
    def get_rerank():
        """
        è·å– Reranker (é‡æ’åº) æ¨¡å‹
        ä½œç”¨: å¯¹æ£€ç´¢å›æ¥çš„ Top-K æ–‡æ¡£è¿›è¡ŒäºŒæ¬¡ç²¾æ’ï¼Œæå‡å‡†ç¡®ç‡ã€‚
        """
        return DashScopeRerank(
            model="gte-rerank",
            api_key=settings.dashscope_api_key,
            top_n=settings.rerank_top_k  # åŠ¨æ€è¯»å– YAML é…ç½®
        )

    @staticmethod
    def get_text_splitter() -> NodeParser:
        """
        [Factory Method] æ–‡æœ¬åˆ‡åˆ†å™¨å·¥å‚

        è®¾è®¡æ„å›¾ (Thesis Point):
        å®ç°äº†ç®—æ³•ç­–ç•¥çš„è§£è€¦ã€‚æ ¹æ®é…ç½®æ–‡ä»¶ä¸­çš„ `chunking_strategy` å­—æ®µï¼Œ
        åŠ¨æ€è¿”å›ä¸åŒçš„åˆ‡åˆ†å™¨å®ä¾‹ï¼Œæ”¯æŒæ¶ˆèå®éªŒ (Ablation Study)ã€‚

        ç­–ç•¥è¯´æ˜:
        1. strategy="fixed" -> TokenTextSplitter
           - åŸç†: æŒ‰å›ºå®šçš„ Token æ•°é‡å¼ºåˆ¶åˆ‡æ–­ã€‚
           - ä¼˜ç‚¹: ä¿è¯æ¯ä¸ªå—çš„å¤§å°ä¸€è‡´ï¼Œè®¡ç®—æ•ˆç‡é«˜ã€‚
           - ç¼ºç‚¹: å®¹æ˜“åœ¨å¥å­ä¸­é—´åˆ‡æ–­ï¼Œå¯¼è‡´è¯­ä¹‰ä¸å®Œæ•´ã€‚
           - åœºæ™¯: ä½œä¸º Baseline (åŸºå‡†) å¯¹ç…§ç»„ã€‚

        2. strategy="recursive" / "sentence" -> SentenceSplitter
           - åŸç†: é€’å½’åœ°å¯»æ‰¾åˆ†éš”ç¬¦ (å¦‚å¥å·ã€æ¢è¡Œ)ï¼Œä¼˜å…ˆä¿æŒå¥å­å®Œæ•´æ€§ã€‚
           - ä¼˜ç‚¹: è¯­ä¹‰è¿è´¯æ€§å¼ºï¼Œåˆ©äº LLM ç†è§£ä¸Šä¸‹æ–‡ã€‚
           - ç¼ºç‚¹: å—å¤§å°ä¼šæœ‰æ³¢åŠ¨ã€‚
           - åœºæ™¯: å®éªŒç»„ (Proposed Method)ã€‚
        """
        strategy = settings.chunking_strategy
        size = settings.chunk_size_child
        overlap = settings.chunk_overlap

        print(f"ğŸ­ [Factory] æ­£åœ¨æ„å»ºåˆ‡ç‰‡å™¨ | ç­–ç•¥: {strategy} | å¤§å°: {size} | é‡å : {overlap}")

        if strategy == "fixed":
            # ç­–ç•¥ A: å›ºå®š Token åˆ‡ç‰‡
            return TokenTextSplitter(
                chunk_size=size,
                chunk_overlap=overlap,
                separator=" "  # ç¡¬åˆ‡åˆ†æ—¶çš„åˆ†éš”ç¬¦
            )

        elif strategy in ["recursive", "sentence"]:
            # ç­–ç•¥ B: é€’å½’/å¥å­åˆ‡ç‰‡ (LlamaIndex é»˜è®¤æ¨è)
            return SentenceSplitter(
                chunk_size=size,
                chunk_overlap=overlap,
                # SentenceSplitter å†…éƒ¨ä¼šè‡ªåŠ¨å¤„ç†ä¸­æ–‡æ ‡ç‚¹ (å¦‚ "ã€‚", "ï¼")
            )

        else:
            # é˜²å¾¡æ€§å›é€€ (Defensive Coding)
            # è™½ç„¶ Settings å·²ç»åšäº†æ ¡éªŒï¼Œä½†å·¥å‚å±‚ä»éœ€å¤„ç†æ„å¤–æƒ…å†µ
            print(f"âš ï¸ [Factory Warning] æœªçŸ¥ç­–ç•¥ '{strategy}'ï¼Œç³»ç»Ÿå°†å›é€€åˆ°é»˜è®¤çš„ TokenTextSplitter")
            return TokenTextSplitter(chunk_size=size, chunk_overlap=overlap)

    @staticmethod
    def warmup_sparse_model():
        """
        [Startup Hook] æ™ºèƒ½åŠ è½½ BGE-M3 ç¨€ç–å‘é‡æ¨¡å‹

        é€»è¾‘:
        1. æ£€æŸ¥å•ä¾‹ç¼“å­˜ï¼Œå¦‚æœå·²åŠ è½½ç›´æ¥è·³è¿‡ã€‚
        2. ä¼˜å…ˆä½¿ç”¨ ModelScope (é˜¿é‡Œæº) ä¸‹è½½ï¼Œè§£å†³å›½å†… HuggingFace è¿æ¥å›°éš¾çš„é—®é¢˜ã€‚
        3. å¦‚æœ ModelScope ä¸å¯ç”¨ï¼Œå›é€€åˆ° HuggingFace é»˜è®¤è¡Œä¸ºã€‚
        """
        if ModelFactory._bgem3_cache is None:
            print("â³ [System] æ­£åœ¨åˆå§‹åŒ–ç¨€ç–æ¨¡å‹ (BGE-M3)...")
            # é»˜è®¤ ID
            model_path_or_id = "BAAI/bge-m3"

            # ğŸ‘‡ æ ¸å¿ƒé›†æˆï¼šè‡ªåŠ¨ä»é˜¿é‡Œäº‘ä¸‹è½½
            if HAS_MODELSCOPE:
                try:
                    print("ğŸš€ [Downloader] æ£€æµ‹åˆ° modelscopeï¼Œæ­£åœ¨é€šè¿‡é˜¿é‡Œäº‘æé€Ÿé€šé“è·å–æ¨¡å‹...")
                    # cache_dir æŒ‡å®šä¸‹è½½åˆ°é¡¹ç›®çš„ resources ç›®å½•ï¼Œæ–¹ä¾¿ç®¡ç†
                    # Xorbits/bge-m3 æ˜¯ BAAI/bge-m3 çš„å®˜æ–¹é•œåƒ
                    model_path_or_id = snapshot_download(
                        'Xorbits/bge-m3',
                        cache_dir='./resources',
                        revision='master'
                    )
                    print(f"âœ… [Downloader] æ¨¡å‹å°±ç»ªï¼Œè·¯å¾„: {model_path_or_id}")
                except Exception as e:
                    print(f"âš ï¸ [Downloader] ModelScope ä¸‹è½½å¼‚å¸¸ (å°†å°è¯•å®˜æ–¹æº): {e}")
            else:
                print("âš ï¸ [Downloader] æœªå®‰è£… modelscopeï¼Œå°†ä½¿ç”¨é»˜è®¤æº (å¯èƒ½è¾ƒæ…¢)...")

            # åŠ è½½æ¨¡å‹
            try:
                # use_fp16=True çœæ˜¾å­˜ (Half Precision)
                ModelFactory._bgem3_cache = BGEM3FlagModel(model_path_or_id, use_fp16=True)
                print("âœ… [System] BGE-M3 åŠ è½½å®Œæˆï¼")
            except Exception as e:
                print(f"âŒ [System] BGE-M3 åŠ è½½å¤±è´¥: {e}")
                raise e

    @staticmethod
    def get_qdrant_sparse_encoders() -> Tuple[Callable, Callable]:
        """
        [Adapter] BGE-M3 -> Qdrant æ ¼å¼é€‚é…å™¨

        Critical Fix (æ ¸å¿ƒä¿®å¤):
        LlamaIndex çš„ Qdrant æ’ä»¶è¦æ±‚ sparse_doc_fn è¿”å› tuple(indices, values)ï¼Œ
        è€Œä¸æ˜¯ list[dict]ã€‚å¦‚æœä¸æ‹†åˆ†ï¼Œä¼šæŠ¥ "too many values to unpack"ã€‚
        æ­¤é€‚é…å™¨è´Ÿè´£å°† FlagEmbedding çš„è¾“å‡ºæ ¼å¼è½¬æ¢ä¸º QdrantClient è¦æ±‚çš„æ ¼å¼ã€‚
        """
        if ModelFactory._bgem3_cache is None:
            ModelFactory.warmup_sparse_model()

        model = ModelFactory._bgem3_cache

        def sparse_doc_fn(texts: List[str]) -> Tuple[List[List[int]], List[List[float]]]:
            """
            æ–‡æ¡£ç¼–ç å™¨ï¼šå°†æ–‡æœ¬åˆ—è¡¨è½¬æ¢ä¸º (indices_list, values_list)
            """
            try:
                # 1. è°ƒç”¨æ¨¡å‹è®¡ç®— (batch)
                output = model.encode(texts, return_dense=False, return_sparse=True, return_colbert_vecs=False)

                batch_indices = []
                batch_values = []

                # 2. éå†ç»“æœï¼Œæ‹†åˆ†ä¸ºç´¢å¼•å’Œæƒé‡ä¸¤ä¸ªç‹¬ç«‹åˆ—è¡¨
                for item in output['lexical_weights']:
                    # item æ˜¯ {str(token_id): float(weight)}
                    indices = []
                    values = []
                    for k, v in item.items():
                        indices.append(int(k))
                        values.append(float(v))

                    batch_indices.append(indices)
                    batch_values.append(values)

                # ğŸ‘‡ è¿”å›ä¸¤ä¸ªåˆ—è¡¨çš„å…ƒç»„ï¼Œè¿™å°±åªæœ‰ 2 ä¸ªå€¼äº†ï¼Œæ»¡è¶³ unpacking
                return batch_indices, batch_values

            except Exception as e:
                print(f"âŒ [BGE-M3 Error] ç¨€ç–å‘é‡è®¡ç®—å‡ºé”™: {e}")
                # å‡ºé”™æ—¶è¿”å›ç©ºåˆ—è¡¨å…ƒç»„ï¼Œé˜²æ­¢å´©æºƒ
                return [[] for _ in texts], [[] for _ in texts]

        def sparse_query_fn(query: str) -> Tuple[List[int], List[float]]:
            """
            æŸ¥è¯¢ç¼–ç å™¨ï¼šå°†å•æ¡æŸ¥è¯¢è½¬æ¢ä¸º (indices, values)
            """
            try:
                # ğŸ‘‡ å¼ºåŠ›æ¸…æ´—ä¸è°ƒè¯•æ‰“å°
                if not isinstance(query, str):
                    query = str(query)
                query = query.strip()

                # å¦‚æœæ˜¯ç©ºå­—ç¬¦ä¸²ï¼Œç›´æ¥è¿”å›ç©ºå‘é‡ï¼ˆå¿…é¡»æ˜¯åˆ—è¡¨çš„åˆ—è¡¨æ ¼å¼ï¼‰
                if not query:
                    print(f"âš ï¸ [BGE-M3 Warning] è·³è¿‡ç©ºæŸ¥è¯¢")
                    return [[]], [[]]

                output = model.encode([query], return_dense=False, return_sparse=True, return_colbert_vecs=False)
                item = output['lexical_weights'][0]

                indices = []
                values = []
                for k, v in item.items():
                    indices.append(int(k))
                    values.append(float(v))

                # Qdrant Query æ¥å£ä¹Ÿè¦æ±‚è§£åŒ…ä¸º 2 ä¸ªå€¼
                # ğŸ‘‡ã€æ ¸å¿ƒä¿®æ”¹ã€‘è¿™é‡Œå¿…é¡»åŒ…ä¸€å±‚ []ï¼Œå˜æˆ List[List]
                return [indices], [values]

            except Exception as e:
                # ğŸ‘‡ æ‰“å°å †æ ˆä»¥ä¾¿è°ƒè¯•
                traceback.print_exc()
                print(f"âŒ [BGE-M3 Error] Query ç¼–ç å‡ºé”™: {e} | Queryå†…å®¹: '{query}'")
                return [[]], [[]]

        return sparse_doc_fn, sparse_query_fn