"""
[Architecture Role: Model Factory (æ¨¡å‹å·¥å‚)]
æ­¤æ¨¡å—è´Ÿè´£ç”Ÿäº§ç³»ç»Ÿæ‰€éœ€çš„æ‰€æœ‰æ¨¡å‹å®ä¾‹ï¼Œå®ç°äº† "å·¥å‚æ¨¡å¼" (Factory Pattern)ã€‚

æ ¸å¿ƒèŒè´£:
1. [LLM & Embedding] ç»Ÿä¸€ç®¡ç†å¤§æ¨¡å‹å’Œå‘é‡æ¨¡å‹çš„åˆå§‹åŒ–å‚æ•°ã€‚
2. [Strategy Dispatch] æ ¹æ®é…ç½®æ–‡ä»¶ (YAML) ä¸­çš„ç­–ç•¥å­—æ®µï¼ŒåŠ¨æ€ç»„è£…ç®—æ³•ç»„ä»¶ã€‚
3. [Singleton Cache] å¯¹é‡å‹æ¨¡å‹ (å¦‚ BGE-M3) è¿›è¡Œå•ä¾‹ç¼“å­˜ï¼Œé¿å…é‡å¤åŠ è½½æ¶ˆè€—æ˜¾å­˜ã€‚

æ ¸å¿ƒå‡çº§ (Phase 6 - Fix & De-bloat):
- [Fix] ä¿®å¤ Embedding Batch Size è¿‡å¤§å¯¼è‡´é˜¿é‡Œäº‘ API æŠ¥é”™ (400 Bad Request) çš„é—®é¢˜ã€‚
- [Optimization] ä¿æŒçº¯ Python å®ç°çš„ `ChineseRecursiveTextSplitter`ï¼Œæ— éœ€ NLTKã€‚
- [Pydantic] ä¿æŒæ­£ç¡®çš„å­—æ®µå£°æ˜ã€‚
"""

from typing import Tuple, Callable, List, Any
import traceback
import re

# ğŸ‘‡ Pydantic ç”¨äºå¤„ç†ç±»å±æ€§
from pydantic import PrivateAttr

# ğŸ‘‡ LlamaIndex æ ¸å¿ƒç»„ä»¶
from llama_index.core.node_parser import (
    NodeParser,
    TokenTextSplitter,
    SemanticSplitterNodeParser
)
from llama_index.core.schema import TextNode
from llama_index.llms.dashscope import DashScope
from llama_index.embeddings.openai import OpenAIEmbedding
from llama_index.postprocessor.dashscope_rerank import DashScopeRerank
from FlagEmbedding import BGEM3FlagModel

from app.settings import settings

try:
    from modelscope import snapshot_download
    HAS_MODELSCOPE = True
except ImportError:
    HAS_MODELSCOPE = False


# --- [Stage 1] ç‰©ç†åˆ‡åˆ†å™¨ (The Atomizer) ---

# ğŸŒŸ [å¸¸é‡] ä¸­æ–‡å¾®åˆ‡åˆ†æ­£åˆ™
CHINESE_SPLIT_REGEX = r'[^ã€‚ï¼ï¼Ÿï¼Œï¼›]+[ã€‚ï¼ï¼Ÿï¼Œï¼›]?'

def chinese_sentence_splitter(text: str) -> List[str]:
    """[å‡½æ•°ç‰ˆ] ä¾› SemanticSplitter ä½¿ç”¨"""
    pattern = re.compile(CHINESE_SPLIT_REGEX)
    segments = [s.strip() for s in pattern.findall(text) if s.strip()]
    return [s for s in segments if len(s) > 1]


# ğŸŒŸ [Classç‰ˆ] çº¯ Python å®ç°çš„é€’å½’åˆ‡åˆ†å™¨ (æ›¿ä»£ SentenceSplitter)
class ChineseRecursiveTextSplitter(NodeParser):
    """
    [No-NLTK Splitter]
    ä¸“é—¨ä¸ºä¸­æ–‡è®¾è®¡çš„é€’å½’åˆ‡åˆ†å™¨ï¼Œä¸ä¾èµ– nltkï¼Œä¸è”ç½‘ã€‚
    """

    # âœ… Pydantic å­—æ®µå£°æ˜
    chunk_size: int
    chunk_overlap: int

    # ç§æœ‰å±æ€§
    _pattern: Any = PrivateAttr()

    def __init__(self, chunk_size: int, chunk_overlap: int, **kwargs):
        super().__init__(
            chunk_size=chunk_size,
            chunk_overlap=chunk_overlap,
            **kwargs
        )
        self._pattern = re.compile(CHINESE_SPLIT_REGEX)

    def _parse_nodes(self, documents, show_progress=False, **kwargs):
        all_nodes = []
        for doc in documents:
            text = doc.text
            if not text: continue

            # 1. æ‰“æ•£ (Atomize)
            segments = [s for s in self._pattern.findall(text) if s.strip()]

            # 2. åˆå¹¶ (Merge with Overlap)
            current_chunk_segs = []
            current_len = 0

            for seg in segments:
                seg_len = len(seg)

                # å¦‚æœåŠ ä¸Šè¿™ä¸€æ®µä¼šçˆ†æ‰ chunk_sizeï¼Œå°±å…ˆç»“ç®—å½“å‰å—
                if current_len + seg_len > self.chunk_size and current_chunk_segs:
                    # ç”Ÿæˆæ–‡æœ¬å—
                    chunk_text = "".join(current_chunk_segs)
                    all_nodes.append(TextNode(text=chunk_text, metadata=doc.metadata))

                    # [Overlap Logic] å¤„ç†é‡å çª—å£
                    backtrack_segs = []
                    backtrack_len = 0
                    for prev_seg in reversed(current_chunk_segs):
                        if backtrack_len + len(prev_seg) < self.chunk_overlap:
                            backtrack_segs.insert(0, prev_seg)
                            backtrack_len += len(prev_seg)
                        else:
                            break

                    # é‡ç½®å½“å‰å—
                    current_chunk_segs = backtrack_segs
                    current_len = backtrack_len

                # åŠ å…¥å½“å‰æ®µ
                current_chunk_segs.append(seg)
                current_len += seg_len

            # å¤„ç†å‰©ä½™çš„å°¾å·´
            if current_chunk_segs:
                chunk_text = "".join(current_chunk_segs)
                all_nodes.append(TextNode(text=chunk_text, metadata=doc.metadata))

        return all_nodes


class ModelFactory:
    _bgem3_cache = None

    @staticmethod
    def get_llm():
        return DashScope(
            model_name=settings.llm_model,
            api_key=settings.dashscope_api_key,
            temperature=0.1
        )

    @staticmethod
    def get_embedding():
        """
        è·å– Dense Embedding (ç¨ å¯†å‘é‡) æ¨¡å‹
        """
        return OpenAIEmbedding(
            model_name=settings.embedding_model,
            api_key=settings.dashscope_api_key,
            api_base="https://dashscope.aliyuncs.com/compatible-mode/v1",
            # ğŸ”´ [FIX] é˜¿é‡Œäº‘ API ç¡¬é™åˆ¶ï¼šBatch Size ä¸èƒ½è¶…è¿‡ 10
            # ä¹‹å‰è®¾ä¸º 100 å¯¼è‡´äº† 400 Bad Request
            embed_batch_size=10,
        )

    @staticmethod
    def get_rerank():
        return DashScopeRerank(
            model="gte-rerank",
            api_key=settings.dashscope_api_key,
            top_n=settings.rerank_top_k
        )

    @staticmethod
    def get_text_splitter() -> NodeParser:
        """
        [Factory Method] æ–‡æœ¬åˆ‡åˆ†å™¨å·¥å‚
        """
        strategy = settings.chunking_strategy
        size = settings.chunk_size_child
        overlap = settings.chunk_overlap

        print(f"ğŸ­ [Factory] æ­£åœ¨æ„å»ºåˆ‡ç‰‡å™¨ | ç­–ç•¥: {strategy}")

        if strategy == "fixed":
            # ç­–ç•¥ A: çº¯æœºæ¢°åˆ‡åˆ†
            return TokenTextSplitter(
                chunk_size=size,
                chunk_overlap=overlap,
                separator=""
            )

        elif strategy in ["recursive", "sentence"]:
            # ç­–ç•¥ B: å¢å¼ºå‹å¥å­åˆ‡åˆ† (ä½¿ç”¨è‡ªå®šä¹‰ç±»)
            print(f"   -> [Stage 1] å¯ç”¨ä¸­æ–‡å¾®åˆ‡åˆ† (No-NLTK Custom Class)")
            print(f"   -> [Stage 2] åˆå¹¶å¤§å°: {size}")

            return ChineseRecursiveTextSplitter(
                chunk_size=size,
                chunk_overlap=overlap
            )

        elif strategy == "semantic":
            # ç­–ç•¥ C: è¯­ä¹‰åˆ†å‰²
            print(f"   -> [Stage 1] å¯ç”¨ä¸­æ–‡å¾®åˆ‡åˆ† (Functionæ¨¡å¼)")
            print(f"   -> [Stage 2] åˆå§‹åŒ–è¯­ä¹‰èšç±»")

            embed_model = ModelFactory.get_embedding()
            buffer_size = settings.semantic_buffer_size
            threshold = settings.semantic_breakpoint_threshold

            return SemanticSplitterNodeParser(
                buffer_size=buffer_size,
                breakpoint_percentile_threshold=threshold,
                embed_model=embed_model,
                sentence_splitter=chinese_sentence_splitter
            )

        else:
            print(f"âš ï¸ [Factory Warning] æœªçŸ¥ç­–ç•¥ '{strategy}'ï¼Œå›é€€åˆ° TokenTextSplitter")
            return TokenTextSplitter(chunk_size=size, chunk_overlap=overlap)

    @staticmethod
    def warmup_sparse_model():
        if ModelFactory._bgem3_cache is None:
            print("â³ [System] æ­£åœ¨åˆå§‹åŒ–ç¨€ç–æ¨¡å‹ (BGE-M3)...")
            model_path_or_id = "BAAI/bge-m3"

            if HAS_MODELSCOPE:
                try:
                    print("ğŸš€ [Downloader] æ­£åœ¨é€šè¿‡é˜¿é‡Œäº‘æé€Ÿé€šé“è·å–æ¨¡å‹...")
                    model_path_or_id = snapshot_download(
                        'Xorbits/bge-m3',
                        cache_dir='./resources',
                        revision='master'
                    )
                except Exception as e:
                    print(f"âš ï¸ [Downloader] ModelScope ä¸‹è½½å¼‚å¸¸: {e}")
            else:
                print("âš ï¸ [Downloader] æœªå®‰è£… modelscopeï¼Œå°†ä½¿ç”¨é»˜è®¤æº...")

            try:
                ModelFactory._bgem3_cache = BGEM3FlagModel(model_path_or_id, use_fp16=True)
                print("âœ… [System] BGE-M3 åŠ è½½å®Œæˆï¼")
            except Exception as e:
                print(f"âŒ [System] BGE-M3 åŠ è½½å¤±è´¥: {e}")
                raise e

    @staticmethod
    def get_qdrant_sparse_encoders() -> Tuple[Callable, Callable]:
        if ModelFactory._bgem3_cache is None:
            ModelFactory.warmup_sparse_model()
        model = ModelFactory._bgem3_cache

        def sparse_doc_fn(texts: List[str]) -> Tuple[List[List[int]], List[List[float]]]:
            try:
                output = model.encode(texts, return_dense=False, return_sparse=True, return_colbert_vecs=False)
                batch_indices = []
                batch_values = []
                for item in output['lexical_weights']:
                    indices = []
                    values = []
                    for k, v in item.items():
                        indices.append(int(k))
                        values.append(float(v))
                    batch_indices.append(indices)
                    batch_values.append(values)
                return batch_indices, batch_values
            except Exception as e:
                print(f"âŒ [BGE-M3 Error] ç¨€ç–å‘é‡è®¡ç®—å‡ºé”™: {e}")
                return [[] for _ in texts], [[] for _ in texts]

        def sparse_query_fn(query: str) -> Tuple[List[int], List[float]]:
            try:
                if not isinstance(query, str): query = str(query)
                query = query.strip()
                if not query: return [[]], [[]]
                output = model.encode([query], return_dense=False, return_sparse=True, return_colbert_vecs=False)
                item = output['lexical_weights'][0]
                indices = []
                values = []
                for k, v in item.items():
                    indices.append(int(k))
                    values.append(float(v))
                return [indices], [values]
            except Exception as e:
                traceback.print_exc()
                print(f"âŒ [BGE-M3 Error] Query ç¼–ç å‡ºé”™: {e}")
                return [[]], [[]]

        return sparse_doc_fn, sparse_query_fn