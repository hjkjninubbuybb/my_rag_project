"""
[Architecture Role: Evaluation Layer (è¯„æµ‹å±‚)]
æ­¤è„šæœ¬ç”¨äºè‡ªåŠ¨åŒ–è¯„ä¼° RAG æ£€ç´¢ç³»ç»Ÿçš„å‡†ç¡®ç‡ï¼Œæ˜¯ "æ¶ˆèå®éªŒ (Ablation Study)" çš„æ ¸å¿ƒå·¥å…·ã€‚

æ ¸å¿ƒå‡çº§ (Engineering):
1. [Visual Alignment] ä½¿ç”¨ unicodedata æ‰‹åŠ¨è®¡ç®—å­—ç¬¦è§†è§‰å®½åº¦ï¼Œè§£å†³ä¸­è‹±æ–‡æ··æ’å¯¼è‡´çš„è¡¨æ ¼é”™ä½é—®é¢˜ã€‚
2. [Zero Dependency] ç§»é™¤å¯¹ tabulate çš„ä¾èµ–ï¼Œçº¯ Python åŸç”Ÿå®ç°å®Œç¾å¯¹é½ã€‚
3. [Metrics] è®¡ç®— Hit Rate, MRR, NDCGã€‚
"""

import sys
import os
import time
import numpy as np
import pandas as pd
import argparse
import unicodedata
from datetime import datetime
from pathlib import Path
from tqdm import tqdm
from typing import List, Dict, Tuple
from dotenv import load_dotenv

# --- 1. ç¯å¢ƒåˆå§‹åŒ– ---
os.environ["no_proxy"] = "localhost,127.0.0.1,::1"

current_file_path = Path(__file__).resolve()
project_root = current_file_path.parent.parent
sys.path.insert(0, str(project_root))

env_path = project_root / ".env"
if env_path.exists():
    load_dotenv(env_path)

from app.core.engine.retrieval import RetrievalService
from app.core.engine.factory import ModelFactory
from app.settings import settings
from llama_index.core.schema import QueryBundle


# --- 2. è§†è§‰å¯¹é½å·¥å…· (æ ¸å¿ƒä¿®å¤) ---

def get_visual_width(s: str) -> int:
    """
    è®¡ç®—å­—ç¬¦ä¸²åœ¨ç»ˆç«¯æ˜¾ç¤ºçš„è§†è§‰å®½åº¦
    ä¸­æ–‡ = 2, è‹±æ–‡/æ•°å­— = 1
    """
    width = 0
    for ch in str(s):
        # East_Asian_Width:
        # 'W' (Wide) = ä¸­æ–‡/æ—¥æ–‡ç­‰
        # 'F' (Fullwidth) = å…¨è§’å­—ç¬¦
        # 'A' (Ambiguous) = æŸäº›ç‰¹æ®Šç¬¦å·ï¼Œé€šå¸¸åœ¨ç»ˆç«¯ä¹Ÿå 2æ ¼
        if unicodedata.east_asian_width(ch) in ('W', 'F', 'A'):
            width += 2
        else:
            width += 1
    return width

def pad_visual(s: str, width: int, align: str = 'left') -> str:
    """
    æ ¹æ®è§†è§‰å®½åº¦è¿›è¡Œå¡«å……
    """
    s = str(s)
    vis_w = get_visual_width(s)
    pad_len = max(0, width - vis_w)

    if align == 'left':
        return s + ' ' * pad_len
    elif align == 'right':
        return ' ' * pad_len + s
    else: # center
        left = pad_len // 2
        right = pad_len - left
        return ' ' * left + s + ' ' * right

def print_aligned_table(data: List[Dict], headers: Dict[str, int]):
    """
    æ‰“å°å®Œç¾å¯¹é½çš„è¡¨æ ¼
    headers: { 'å­—æ®µå': ç›®æ ‡åˆ—å®½ }
    """
    # 1. æ‰“å°è¡¨å¤´
    header_row = "|"
    for title, width in headers.items():
        header_row += f" {pad_visual(title, width, 'center')} |"

    border = "+" + "+".join(["-" * (w + 2) for w in headers.values()]) + "+"

    print(border)
    print(header_row)
    print(border)

    # 2. æ‰“å°æ•°æ®è¡Œ
    for item in data:
        row_str = "|"
        for key, width in headers.items():
            val = item.get(key, "")
            # æ•°å­—é å³ï¼Œæ–‡å­—é å·¦/å±…ä¸­
            align = 'right' if isinstance(val, (int, float)) and 'Rate' not in key else 'center'
            row_str += f" {pad_visual(val, width, align)} |"
        print(row_str)

    print(border)


# --- 3. æ•°å­¦å·¥å…· ---

def calculate_cosine_similarity(vec1: List[float], vec2: List[float]) -> float:
    v1 = np.array(vec1)
    v2 = np.array(vec2)
    norm1 = np.linalg.norm(v1)
    norm2 = np.linalg.norm(v2)
    if norm1 == 0 or norm2 == 0:
        return 0.0
    return np.dot(v1, v2) / (norm1 * norm2)

def calculate_ndcg(k: int, relevance_scores: List[int]) -> float:
    scores = relevance_scores[:k]
    dcg = 0.0
    for i, rel in enumerate(scores):
        if rel > 0:
            dcg += rel / np.log2(i + 2)
    ideal_scores = sorted(scores, reverse=True)
    idcg = 0.0
    for i, rel in enumerate(ideal_scores):
        if rel > 0:
            idcg += rel / np.log2(i + 2)
    if idcg == 0:
        return 0.0
    return dcg / idcg


# --- 4. è¯­ä¹‰è£åˆ¤ ---

class SemanticJudge:
    def __init__(self):
        print("âš–ï¸ [Judge] åˆå§‹åŒ–è¯­ä¹‰è£åˆ¤...")
        self.embed_model = ModelFactory.get_embedding()
        self._cache = {}

    def get_embedding(self, text: str) -> List[float]:
        if text in self._cache:
            return self._cache[text]
        emb = self.embed_model.get_text_embedding(text)
        self._cache[text] = emb
        return emb

    def is_hit(self, ground_truth: str, retrieved_text: str, threshold: float = 0.85) -> bool:
        def clean(t):
            return str(t).replace(" ", "").replace("\n", "").lower()

        if clean(ground_truth) in clean(retrieved_text):
            return True

        try:
            vec_gt = self.get_embedding(ground_truth)
            vec_ret = self.get_embedding(retrieved_text)
            return calculate_cosine_similarity(vec_gt, vec_ret) > threshold
        except:
            return False


# --- 5. å®éªŒæ§åˆ¶å™¨ ---

class ExperimentRunner:
    def __init__(self):
        print(f"\nğŸ› ï¸ [System] åˆå§‹åŒ–è¯„æµ‹ | å®éªŒID: {settings.experiment_id}")
        print(f"   -> é›†åˆ: {settings.collection_name}")
        self.service = RetrievalService()
        self.judge = SemanticJudge()
        self.reranker = self.service.reranker

        # æ£€æŸ¥é›†åˆ
        try:
            client = self.service.store_manager.client
            if client.collection_exists(settings.collection_name):
                cnt = client.count(settings.collection_name).count
                if cnt == 0:
                    print(f"âš ï¸ [è­¦å‘Š] é›†åˆ '{settings.collection_name}' ä¸ºç©ºï¼è¯·å…ˆè¿è¡Œ main.py å…¥åº“ã€‚")
                else:
                    print(f"âœ… [Check] é›†åˆåŒ…å« {cnt} æ¡æ•°æ®")
        except:
            pass

        self.configs = [
            {"name": "A", "desc": "çº¯å‘é‡ (Baseline)", "hybrid": False, "merge": False, "rerank": False},
            {"name": "B", "desc": "æ— æ··åˆæ£€ç´¢ (No Hybrid)", "hybrid": False, "merge": True, "rerank": True},
            {"name": "C", "desc": "æ— é‡æ’åº (No Rerank)", "hybrid": True, "merge": True, "rerank": False},
            {"name": "D", "desc": "å®Œå…¨ä½“ (Full Pipeline)", "hybrid": True, "merge": True, "rerank": True},
        ]

    def run_experiment(self, config: Dict, dataset: pd.DataFrame) -> Tuple[Dict, List[Dict]]:
        exp_tag = f"Exp_{config['name']}"
        print(f"\nğŸ§ª å¯åŠ¨å­å®éªŒ [{config['name']}]: {config['desc']}")

        retriever = self.service.get_retriever(
            enable_hybrid=config["hybrid"],
            enable_merge=config["merge"]
        )

        detailed_results = []
        start_time = time.time()
        top_k = settings.rerank_top_k

        for idx, row in tqdm(dataset.iterrows(), total=len(dataset), unit="é¢˜", desc=exp_tag):
            query = str(row['Question'])
            ground_truth = str(row['Ground Truth Text'])
            category = str(row.get('Category', 'Unknown'))

            try:
                nodes = retriever.retrieve(query)

                if config["rerank"] and self.reranker:
                    query_bundle = QueryBundle(query_str=query)
                    ranked_nodes = self.reranker.postprocess_nodes(nodes, query_bundle)
                    final_nodes = ranked_nodes[:top_k]
                else:
                    final_nodes = nodes[:top_k]

                relevance_scores = []
                hit_rank = -1

                # Top 5
                retrieved_snippets = []
                if final_nodes:
                    for i, node in enumerate(final_nodes[:5]):
                        clean_text = node.text[:80].replace("\n", " ").replace("\r", " ")
                        retrieved_snippets.append(f"[{i + 1}] {clean_text}...")
                    top5_text_combined = "\n".join(retrieved_snippets)
                else:
                    top5_text_combined = "æ— ç»“æœ"

                for rank, node in enumerate(final_nodes):
                    is_hit = self.judge.is_hit(ground_truth, node.text)
                    relevance_scores.append(1 if is_hit else 0)
                    if is_hit and hit_rank == -1:
                        hit_rank = rank + 1

                if len(relevance_scores) < top_k:
                    relevance_scores += [0] * (top_k - len(relevance_scores))

                is_hit_int = 1 if hit_rank > 0 else 0
                mrr = 1.0 / hit_rank if hit_rank > 0 else 0.0
                ndcg = calculate_ndcg(top_k, relevance_scores)

                detailed_results.append({
                    "Experiment": config["name"],
                    "Category": category,
                    "Question": query,
                    "Is_Hit": is_hit_int,
                    "MRR": mrr,
                    "NDCG": ndcg,
                    "Ground_Truth": ground_truth,
                    "Retrieved_Top5": top5_text_combined
                })

            except Exception as e:
                print(f"âŒ Error: {e}")
                detailed_results.append({
                    "Experiment": config["name"],
                    "Category": category,
                    "Question": query,
                    "Is_Hit": 0, "MRR": 0, "NDCG": 0,
                    "Ground_Truth": ground_truth,
                    "Retrieved_Top5": f"Error: {e}"
                })

        avg_latency = ((time.time() - start_time) * 1000) / len(dataset)

        metrics = {
            "Experiment": config["name"],
            "Description": config["desc"],
            "Hit_Rate": f"{pd.DataFrame(detailed_results)['Is_Hit'].mean():.4f}",
            "MRR": f"{pd.DataFrame(detailed_results)['MRR'].mean():.4f}",
            "NDCG": f"{pd.DataFrame(detailed_results)['NDCG'].mean():.4f}",
            "Latency": f"{avg_latency:.1f} ms"
        }

        return metrics, detailed_results

    def run(self, limit: int = None, target_exp: str = None):
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        exp_id_safe = settings.experiment_id.replace(" ", "_")

        output_dir = project_root / "tests" / "data" / "reports"
        output_dir.mkdir(parents=True, exist_ok=True)

        suffix = f"_limit{limit}" if limit else "_full"
        summary_file = output_dir / f"report_{exp_id_safe}_summary_{timestamp}{suffix}.csv"
        details_file = output_dir / f"report_{exp_id_safe}_details_{timestamp}{suffix}.csv"

        data_path = project_root / "tests" / "data" / "test_dataset.csv"
        if not data_path.exists():
            print(f"âŒ æ‰¾ä¸åˆ°æµ‹è¯•é›†: {data_path}")
            return

        try:
            df = pd.read_csv(data_path, encoding='utf-8-sig')
        except:
            df = pd.read_csv(data_path, encoding='gbk')

        if limit:
            print(f"âœ‚ï¸  Debugæ¨¡å¼: å‰ {limit} æ¡")
            df = df.head(limit)

        experiments_to_run = self.configs
        if target_exp:
            experiments_to_run = [c for c in self.configs if c["name"].lower() == target_exp.lower()]

        all_metrics = []
        all_details = []

        for config in experiments_to_run:
            try:
                metrics, details = self.run_experiment(config, df)
                all_metrics.append(metrics)
                all_details.extend(details)
            except Exception as e:
                print(f"âŒ å®éªŒ {config['name']} å¤±è´¥: {e}")

        if all_metrics:
            print("\n")
            print("=" * 100)
            print(f"ğŸ† æ¶ˆèå®éªŒæŠ¥å‘Š | ID: {settings.experiment_id}")

            # --- æ‰‹åŠ¨å¯¹é½è¡¨æ ¼æ‰“å° ---
            # å®šä¹‰æ¯ä¸€åˆ—çš„è§†è§‰å®½åº¦
            headers = {
                "Experiment": 12,
                "Description": 30, # ç»™æè¿°ç•™å®½ä¸€ç‚¹
                "Hit_Rate": 10,
                "MRR": 10,
                "NDCG": 10,
                "Latency": 12
            }
            print_aligned_table(all_metrics, headers)

            print("=" * 100)

            # ä¿å­˜ CSV
            final_df = pd.DataFrame(all_metrics)
            final_df.to_csv(summary_file, index=False, encoding='utf-8-sig')
            details_df = pd.DataFrame(all_details)
            details_df.to_csv(details_file, index=False, encoding='utf-8-sig')
            print(f"\nâœ… æŠ¥è¡¨å·²ä¿å­˜è‡³: {output_dir}")
        else:
            print("\nâš ï¸ æ— ç»“æœç”Ÿæˆ")


def parse_args():
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", type=str, default="configs/default.yaml", help="æŒ‡å®šé…ç½®")
    parser.add_argument("--limit", type=int, default=None, help="ä»…æµ‹è¯•å‰ N æ¡")
    parser.add_argument("--exp", type=str, default=None, help="æŒ‡å®šå­å®éªŒ (A/B/C/D)")
    return parser.parse_args()


if __name__ == "__main__":
    args = parse_args()
    try:
        settings.load_experiment_config(args.config)
        settings.qdrant_path = str(project_root / settings.qdrant_path)
    except Exception as e:
        print(f"âŒ é…ç½®åŠ è½½å¤±è´¥: {e}")
        sys.exit(1)

    runner = ExperimentRunner()
    runner.run(limit=args.limit, target_exp=args.exp)