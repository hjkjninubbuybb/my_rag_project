import sys
import os
import time
import numpy as np
import pandas as pd
import argparse
from datetime import datetime  # ğŸ‘ˆ [æ–°å¢] ç”¨äºç”Ÿæˆæ—¶é—´æˆ³
from pathlib import Path
from tqdm import tqdm
from typing import List, Dict, Tuple
from dotenv import load_dotenv

# --- 1. ç¯å¢ƒåˆå§‹åŒ– ---

current_file_path = Path(__file__).resolve()
project_root = current_file_path.parent.parent
sys.path.insert(0, str(project_root))

env_path = project_root / ".env"
if env_path.exists():
    load_dotenv(env_path)
else:
    print(f"âš ï¸ è­¦å‘Š: æœªæ‰¾åˆ° .env æ–‡ä»¶")

from app.core.engine.retrieval import RetrievalService
from app.core.engine.factory import ModelFactory
from app.settings import settings
from llama_index.core.schema import QueryBundle

# å¼ºåˆ¶ä¿®æ­£ Qdrant è·¯å¾„ (å¦‚æœä½ ä¿®æ”¹äº† .env é‡Œçš„è·¯å¾„ï¼Œè¯·æ³¨é‡Šæ‰ä¸‹é¢ 3 è¡Œ)
qdrant_abs_path = project_root / "qdrant_db"
if hasattr(settings, "qdrant_path"):
    settings.qdrant_path = str(qdrant_abs_path)


# --- 2. æ•°å­¦å·¥å…· ---

def calculate_cosine_similarity(vec1: List[float], vec2: List[float]) -> float:
    v1 = np.array(vec1)
    v2 = np.array(vec2)
    norm1 = np.linalg.norm(v1)
    norm2 = np.linalg.norm(v2)
    if norm1 == 0 or norm2 == 0:
        return 0.0
    return np.dot(v1, v2) / (norm1 * norm2)


def calculate_ndcg(k: int, relevance_scores: List[int]) -> float:
    scores = relevance_scores[:k]
    dcg = 0.0
    for i, rel in enumerate(scores):
        if rel > 0:
            dcg += rel / np.log2(i + 2)
    ideal_scores = sorted(scores, reverse=True)
    idcg = 0.0
    for i, rel in enumerate(ideal_scores):
        if rel > 0:
            idcg += rel / np.log2(i + 2)
    if idcg == 0:
        return 0.0
    return dcg / idcg


# --- 3. è£åˆ¤ç±» ---

class SemanticJudge:
    def __init__(self):
        print("âš–ï¸ åˆå§‹åŒ–è¯­ä¹‰è£åˆ¤...")
        self.embed_model = ModelFactory.get_embedding()
        self._cache = {}

    def get_embedding(self, text: str) -> List[float]:
        if text in self._cache:
            return self._cache[text]
        emb = self.embed_model.get_text_embedding(text)
        self._cache[text] = emb
        return emb

    def is_hit(self, ground_truth: str, retrieved_text: str, threshold: float = 0.85) -> bool:
        def clean(t):
            return str(t).replace(" ", "").replace("\n", "").lower()

        if clean(ground_truth) in clean(retrieved_text):
            return True
        try:
            vec_gt = self.get_embedding(ground_truth)
            vec_ret = self.get_embedding(retrieved_text)
            return calculate_cosine_similarity(vec_gt, vec_ret) > threshold
        except:
            return False


# --- 4. å®éªŒæ§åˆ¶å™¨ ---

class ExperimentRunner:
    def __init__(self):
        self.service = RetrievalService()
        self.judge = SemanticJudge()
        self.reranker = self.service.reranker

        self.configs = [
            {"name": "A", "desc": "çº¯å‘é‡ (Baseline)", "hybrid": False, "merge": False, "rerank": False},
            {"name": "B", "desc": "æ··åˆæ£€ç´¢ (Hybrid)", "hybrid": True, "merge": False, "rerank": False},
            {"name": "C", "desc": "è‡ªåŠ¨åˆå¹¶ (Auto-Merge)", "hybrid": True, "merge": True, "rerank": False},
            {"name": "D", "desc": "å®Œå…¨ä½“ (Full)", "hybrid": True, "merge": True, "rerank": True},
        ]

    def run_experiment(self, config: Dict, dataset: pd.DataFrame) -> Tuple[Dict, List[Dict]]:
        """
        è¿è¡Œå•ç»„å®éªŒ
        è¿”å›: (ç»Ÿè®¡æŒ‡æ ‡metrics, è¯¦ç»†è®°å½•details)
        """
        exp_tag = f"Exp_{config['name']}"
        print(f"\nğŸ§ª å¯åŠ¨å®éªŒ [{config['name']}]: {config['desc']}")

        retriever = self.service.get_retriever(
            enable_hybrid=config["hybrid"],
            enable_merge=config["merge"]
        )

        detailed_results = []  # è®°å½•æ¯ä¸€é¢˜çš„è¯¦ç»†æƒ…å†µ
        start_time = time.time()
        top_k = settings.rerank_top_k

        for idx, row in tqdm(dataset.iterrows(), total=len(dataset), unit="é¢˜", desc=exp_tag):
            query = str(row['Question'])
            ground_truth = str(row['Ground Truth Text'])
            category = str(row.get('Category', 'Unknown'))

            # æ£€ç´¢
            nodes = retriever.retrieve(query)

            # é‡æ’
            if config["rerank"] and self.reranker:
                query_bundle = QueryBundle(query_str=query)
                ranked_nodes = self.reranker.postprocess_nodes(nodes, query_bundle)
                final_nodes = ranked_nodes[:top_k]
            else:
                final_nodes = nodes[:top_k]

            # åˆ¤åˆ†
            relevance_scores = []
            hit_rank = -1

            # ğŸ‘‡ [ä¿®æ”¹] å¢åŠ åˆ¤ç©ºæ£€æŸ¥ï¼Œé˜²æ­¢ list index out of range
            if final_nodes and len(final_nodes) > 0:
                top1_text = final_nodes[0].text
            else:
                top1_text = "æ— ç»“æœ"

            for rank, node in enumerate(final_nodes):
                is_hit = self.judge.is_hit(ground_truth, node.text)
                relevance_scores.append(1 if is_hit else 0)
                if is_hit and hit_rank == -1:
                    hit_rank = rank + 1

            if len(relevance_scores) < top_k:
                relevance_scores += [0] * (top_k - len(relevance_scores))

            # è®¡ç®—å•é¢˜æŒ‡æ ‡
            is_hit_int = 1 if hit_rank > 0 else 0
            mrr = 1.0 / hit_rank if hit_rank > 0 else 0.0
            ndcg = calculate_ndcg(top_k, relevance_scores)

            # è®°å½•è¯¦æƒ… (ç”¨äºå†™ Case Study)
            detailed_results.append({
                "Experiment": config["name"],
                "Category": category,
                "Question": query,
                "Is_Hit": is_hit_int,
                "MRR": mrr,
                "NDCG": ndcg,
                "Ground_Truth": ground_truth,
                "Top1_Retrieved": top1_text[:100] + "..."  # åªå­˜å‰100å­—é¢„è§ˆ
            })

        avg_latency = ((time.time() - start_time) * 1000) / len(dataset)
        df_res = pd.DataFrame(detailed_results)

        metrics = {
            "Experiment": config["name"],
            "Description": config["desc"],
            "Hit_Rate": df_res["Is_Hit"].mean(),
            "MRR": df_res["MRR"].mean(),
            "NDCG": df_res["NDCG"].mean(),
            "Latency(ms)": avg_latency
        }

        return metrics, detailed_results

    def run(self, limit: int = None, target_exp: str = None):
        # ç”Ÿæˆå¸¦æ—¶é—´æˆ³çš„æ–‡ä»¶å (æ ¸å¿ƒä¿®æ”¹)
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = project_root / "tests" / "data" / "reports"
        output_dir.mkdir(parents=True, exist_ok=True)  # è‡ªåŠ¨åˆ›å»º reports æ–‡ä»¶å¤¹

        suffix = f"_limit{limit}" if limit else "_full"
        summary_file = output_dir / f"report_summary_{timestamp}{suffix}.csv"
        details_file = output_dir / f"report_details_{timestamp}{suffix}.csv"

        # åŠ è½½æ•°æ®
        data_path = project_root / "tests" / "data" / "test_dataset.csv"
        if not data_path.exists():
            print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ° {data_path}")
            return

        try:
            df = pd.read_csv(data_path, encoding='utf-8-sig')
        except:
            df = pd.read_csv(data_path, encoding='gbk')

        if limit:
            print(f"âœ‚ï¸  æµ‹è¯•æ¨¡å¼: ä»…æˆªå–å‰ {limit} æ¡")
            df = df.head(limit)

        print(f"ğŸ“Š æµ‹è¯•é›†: {len(df)} æ¡ | ğŸ•’ ä»»åŠ¡ID: {timestamp}")

        # å®éªŒç­›é€‰
        experiments_to_run = self.configs
        if target_exp:
            experiments_to_run = [c for c in self.configs if c["name"].lower() == target_exp.lower()]
            if not experiments_to_run:
                print(f"âŒ é”™è¯¯: æœªæ‰¾åˆ°å®éªŒ '{target_exp}'")
                return

        # è¿è¡Œ
        all_metrics = []
        all_details = []

        for config in experiments_to_run:
            # ğŸ‘‡ [ä¿®æ”¹] æš‚æ—¶æ³¨é‡Šæ‰ try-except ä»¥ä¾¿æ˜¾ç¤ºè¯¦ç»†æŠ¥é”™
            # try:
            metrics, details = self.run_experiment(config, df)
            all_metrics.append(metrics)
            all_details.extend(details)
            print(f"   ğŸ‘‰ ç»“æœ: Hit={metrics['Hit_Rate']:.2%} | MRR={metrics['MRR']:.4f}")
            # except Exception as e:
            #     print(f"âŒ å®éªŒ {config['name']} å¤±è´¥: {e}")

        # ä¿å­˜ç»“æœ (æ— è®ºæ˜¯å¦ limitï¼Œéƒ½ä¿å­˜ï¼)
        if all_metrics:
            # 1. ä¿å­˜æ±‡æ€»è¡¨
            final_df = pd.DataFrame(all_metrics)
            print("\n" + "=" * 80)
            print("ğŸ† å®éªŒæ±‡æ€»æŠ¥å‘Š")
            print("=" * 80)
            print(final_df.to_string(index=False, float_format=lambda x: "{:.4f}".format(x)))

            final_df.to_csv(summary_file, index=False, encoding='utf-8-sig')
            print(f"\nâœ… æ±‡æ€»æŠ¥è¡¨å·²ä¿å­˜: {summary_file}")

            # 2. ä¿å­˜æ˜ç»†è¡¨ (Case Study ç´ æ)
            details_df = pd.DataFrame(all_details)
            details_df.to_csv(details_file, index=False, encoding='utf-8-sig')
            print(f"âœ… è¯¦ç»†è®°å½•å·²ä¿å­˜: {details_file}")
            print(f"   (åŒ…å«æ¯ä¸€é“é¢˜çš„å¾—åˆ†å’Œæ£€ç´¢å†…å®¹ï¼Œå¯ç”¨äºè®ºæ–‡æ¡ˆä¾‹åˆ†æ)")
        else:
            print("\nâš ï¸ æ— å®éªŒç»“æœç”Ÿæˆ")


if __name__ == "__main__":
    os.environ["no_proxy"] = "localhost,127.0.0.1"

    parser = argparse.ArgumentParser()
    parser.add_argument("--limit", type=int, default=None, help="ä»…æµ‹è¯•å‰ N æ¡")
    parser.add_argument("--exp", type=str, default=None, help="æŒ‡å®šå®éªŒ (A/B/C/D)")

    args = parser.parse_args()

    runner = ExperimentRunner()
    runner.run(limit=args.limit, target_exp=args.exp)